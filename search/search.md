# SolitonNKで検索する

Gravwellは構造読み取りデータレイクであり、検索パイプラインはその機能の中核です。 ここで、プラットフォームのパワーと柔軟性を確認できます。収集時にデータについて多くを知る必要はありません。 代わりに、生のデータを取り込んで検索処理パイプラインを組み立て、その生のデータを実用的な結果に変えることができます。 これにより、SIEMで一般的な問題である、特定の金型に合わせようとするデータの正規化またはマッサージがほとんどなくなります。 さらに、新しい情報、手法、または検索モジュールが利用可能になったときに、古いデータに対して遡及的に分析を実行できます。

検索パイプラインはGravwellの機能の中核であり、Linux/Unixコマンドラインと同様の方法で動作します。Gravwellの検索クエリ構文は、生データを検索して結果に変換するための処理パイプラインを組み立てます。パイプラインモジュールは、多くの場合、少なくとも1つの抽出モジュール、1つ以上のフィルタリングまたは処理モジュール、およびレンダリングモジュールで構成されています。例えば、以下のクエリは「weather」とタグ付けされたデータを取得し、jsonモジュールを使用して温度と地名を抽出し、evalモジュールを使用して温度が50度以下のエントリーをフィルタリングし、最後にテーブルレンダラーを使用して名前と温度フィールドの内容を表示します。

```
tag=weather json main.temp name | eval temp > 50 | table name temp
```

![](weather-extract.png)

クエリを実行するために使用されるユーザーインターフェイスのドキュメントは[ここにあります](#!gui/queries/queries.md)。

<video controls>
	<source src="https://www.gravwell.io/hubfs/videos/Query%20Basics.mp4" type="video/mp4">
	Your browser does not support the video tag
</video>

## エントリー

エントリーはGravwellにおけるデータの基本単位です。エントリーはインジェスターによって生成され、インデクサーに送られ、問い合わせがあるまで保存されます。各エントリーには4つのフィールドがあります：DATA、TIMESTAMP、SRC、TAGです。

![](entry.png)

TAGフィールドは、特定のエントリを分類します。ネットフローレコードは「netflow」というタグが付けられているかもしれませんし、ウェブサーバからのsyslogレコードは「syslog-www」というタグが付けられているかもしれません。SRCフィールドは、エントリがどこから発信されたかを示すIPアドレスです。TIMESTAMPフィールドは、エントリーがいつ作成されたかを示します。DATAフィールドには、エントリーの実際の本体が含まれています。DATAフィールドに入るものに制限はありません。Gravwellはどんなバイナリデータも受け入れて保存します。

上記の例のクエリでは、"weather" とタグ付けされたエントリーを扱っており、そのDATAフィールドには以下のようなJSONが含まれています。

```
{"coord":{"lon":-116.8,"lat":47.69},"weather":[{"id":801,"main":"Clouds","description":"few clouds","icon":"02d"}],"base":"stations","main":{"temp":40.91,"feels_like":34.56,"temp_min":39.2,"temp_max":42.01,"pressure":1032,"humidity":80},"visibility":10000,"wind":{"speed":5.82,"deg":200},"clouds":{"all":20},"dt":1605896400,"sys":{"type":1,"id":3672,"country":"US","sunrise":1605884460,"sunset":1605917119},"timezone":-28800,"id":5589173,"name":"Coeur d'Alene","cod":200}
```
```
{"coord":{"lon":-106.65,"lat":35.08},"weather":[{"id":800,"main":"Clear","description":"clear sky","icon":"01d"}],"base":"stations","main":{"temp":62.94,"feels_like":56.34,"temp_min":61,"temp_max":66,"pressure":1029,"humidity":27},"visibility":10000,"wind":{"speed":4.52,"deg":224},"clouds":{"all":1},"dt":1605896230,"sys":{"type":1,"id":3232,"country":"US","sunrise":1605880012,"sunset":1605916694},"timezone":-25200,"id":5454711,"name":"Albuquerque","cod":200}
```

クエリはこの生のJSONを受け取り、分析のために構造を適用します。

## クエリの構成要素

上に例示したクエリは、以下に説明するように、いくつかの個別の部分に分割することができます。

### タグの仕様

Gravwellのクエリの最初の部分はタグの指定です。ユーザーは、1つ以上のタグを指定しなければならず、そこからインデクサーがデータを引っ張ってきてパイプラインの開始に投入します。以下の例では、"reddit" というタグが付けられたデータを取得するようにGravwellに指示しています。

```
tag=reddit
```

複数のタグを指定することもできます：

```
tag=reddit,hackernews
```

また、ワイルドカードを使用することもできます：

```
tag=syslog-http-*,syslog-firewall-*
```

これらはすべて有効なクエリです。タグ指定だけのクエリを実行した場合、Gravwellはエントリーを取得し、処理されていない状態でコンテンツを表示します。

![](tag-only.png)

### 抽出モジュール

抽出モジュールは、基礎となる生のデータからフィールド/値/プロパティを引き出すために使用されます。先ほど示した天気のデータを含むエントリーがあれば、以下のクエリを使用して地名と気温を引き出し、気温が50度以上のエントリーのみを表示します。

```
tag=weather json main.temp name | eval temp > 50 | table name temp
```

上の例では、`JSON`モジュールは生のJSONテキストから値を抽出し、パイプラインの下のモジュールに「列挙された値」として提供しています。言い換えれば、このモジュールはJSONを解析して、ユーザーが必要とする特定のコンポーネントを抽出します。「name」と「temp」というフィールドを抽出し、後者は「main」フィールド内で見つけられます。

これらのモジュールの完全なドキュメントは、[抽出モジュール](extractionmodules.md)にあります。

#### フィルタリング

Gravwellの抽出モジュールは、通常、抽出時に抽出されたアイテムをフィルタリングできるようにします。フィルタリングでは、特定の基準に一致するかどうかに基づいて、エントリーをドロップしたりパスしたりすることができます。完全なドキュメントについては、[このページ](filtering.md)を参照してください。


### 処理モジュール

処理モジュールは、データエントリーの分析、望ましくないデータのフィルタリング、統計分析の実行などに使用されます。検索パイプラインは、前のモジュールの結果に基づいて動作する多数の処理モジュールを含んでいても良いです。

前節の例では、`eval` はフィルタリングを行う処理モジュールです。多くの処理モジュールは、モジュールの呼び出しの前に additional タグ指定を置くことで、全体的なクエリタグのサブセットで動作するようにすることができることに注意してください。我々はこれをデータフュージョンと呼んでいます。これは、初期の抽出を別の方法で行う必要がある場合に非常に便利です。

```
tag=reddit,hackernews tag=reddit json Body | tag=hackernews json body as Body | eval len(Body) < 20 | table Body
```

[処理モジュールの完全なドキュメントについては、ここをクリックしてください。](processingmodules.md)


### レンダリングモジュール

レンダリングモジュールは、検索モジュールによって生成された結果を取得し、ユーザーにグラフィカルに表示します。検索パイプラインには、パイプラインの最後に1つのレンダリングモジュールのみが含まれます。

```
tag=weather json main.temp name | eval temp > 50 | table name temp
```

上の例では、"table" はレンダリングモジュールです。"name" と "temp" 列挙値の内容をそれぞれの列に表示するように指示しています。

[レンダリングモジュールの完全なドキュメントについてはここをクリックしてください。](rendermodules.md)


## 列挙値

列挙値は、検索パイプライン内で作成され使用される特殊なデータ要素です。以下のパイプラインでは、いくつかの列挙値が作成されています。

```
tag=reddit json Body | langfind -e Body | count by lang | sort by count desc | table lang count
```

まず、jsonモジュールは生のエントリのJSONを解析して "Body" 要素を取り出し、`Body`という名前の列挙値に格納します。次にlangfindモジュールは `Body` の列挙値にアクセスし、使用された言語の解析を試みます。その結果を `lang` という名前の新しい列挙値に格納します。次に、countモジュールは `lang` 列挙値を読み込み、それぞれの値が何回出現したかを数え、その結果を `count` という名前の列挙値に格納します。パイプラインの残りの部分はカウント数に基づいて結果をソートし、`lang`と`count`の列挙値からテーブルを作成します。

列挙値は、クエリの過程でエントリーに添付される追加のフィールドと考えることができます。netflowモジュールを使用して生のDATAフィールドからSrc値とDst値を抽出する場合、SrcフィールドとDstフィールドをエントリに追加します：

```
tag=netflow netflow Src Dst
```

![](entry-enumerated.png)

ドキュメント全体のさらなる例は、列挙値の使用を明確にするのに役立つはずです。

## 引用とトークン化

Gravwell モジュールに引数を指定するときは、特殊文字に注意してください。ほとんどのモジュールでは、スペース、タブ、改行、次の文字を区切り文字として扱います：!#$%&'()*+,-./:;<=>?@

これらの文字のいずれかを含むモジュールへの引数を指定する場合は、引数をダブルクォートで囲みます：

```
json "search-id"
```

```
grep "dank memes"
```

二重引用符を使用する必要がある場合は、エスケープすることができます。たとえば、テキスト内の誤用されたダイアログタグを識別するために、シーケンス `",` を検索できます：

```
grep "\","
```

## マクロ

マクロは、長くて複雑なクエリを覚えやすいショートカットに変えるのに役立ちます。詳細は [完全なマクロドキュメント](#!search/macros.md) を参照してください。

## 複合クエリ

複数のデータソースを活用したり、データを別のクエリに融合したり、複雑なクエリを簡素化したりするために、複数のクエリを1つの「複合」クエリとして組み合わせることができます。Gravwellの複合クエリ構文は、単純な順序のクエリのシーケンスですが、シーケンスの後のクエリで参照できる一時的なリソースを作成するための追加の記法を備えています。

複合クエリは、メインクエリ（シーケンスの最後のクエリ）と1つ以上の内部クエリから構成されます。メインクエリは通常のクエリと同じように記述されますが、内部クエリは常に `@<identifier>{<query>}` という記法でラップされます。クエリは `;` で区切られています。

内部クエリは `@<identifier>` の形式で名前付きリソースを生成します。これらのリソースは、以下に列挙するテーブルベースのリソースをサポートする任意のモジュールで通常のリソースとして使用することができます。

| モジュール | 注意事項 |
|--------|-------|
| [dump](#!search/dump/dump.md) | |
| [enrich](#!search/enrich/enrich.md) | |
| [ipexist](#!search/ipexist/ipexist.md) | 内部クエリは `-format ipexist` フラグを持つテーブルモジュールを使用しなければなりません |
| [iplookup](#!search/iplookup/iplookup.md) | 内部クエリは `-format csv` フラグを持つテーブルモジュールを使用しなければなりません |
| [lookup](#!search/lookup/lookup.md) | |
| [anko](#!search/anko/anko.md) | Ankoスクリプトは、指定されたリソースから読み込むことができます |

名前付きリソースは、それらが存在する複合クエリにスコープされ、一時的です。複合クエリ内の他のクエリにのみアクセスでき、クエリが完了するとすぐに削除されます。

例えば、「dns」と「conns」というタグの下にDNSクエリとIPレベルの接続データの両方があり、最初に対応するDNSクエリを持たない接続だけに接続データをフィルタリングしたいとします。複合クエリを使用して、最初のクエリにDNSデータを追加してフィルタリングすることができます。

まずは内部のクエリから見てみましょう：

```
tag=dns json query answers | table query answers
```

これで表ができます。

![](compound-ex1.png)

内部クエリでは、DNSデータ内のすべてのクエリと回答のテーブルを作成します。これは内部クエリなので、後のクエリがその出力を参照できるように名前を付け、中括弧でクエリを括る必要があります。この内部クエリを「DNS」と呼ぶことにします。

```
@dns{tag=dns json query answers | table query answers}
```

メインクエリでは、接続データを使用し、内部クエリ "@dns" から読み込むために `lookup` モジュールを使用します。

```
tag=conns json SrcIP DstIP SrcIPBytes DstIPBytes | lookup -s -v -r @dns SrcIP answers query
| table SrcIP DstIP SrcIPBytes DstIPBytes
```

このクエリは `lookup` モジュールを使用して、DNSの応答にマッチするSrcIPを持つ connsデータのエントリを ( `-s` と `-v` フラグを使って) ドロップします。そこからデータのテーブルを作成します。

これを単純に結合して `;` で区切ることで、複合クエリに変換します：

```
@dns{
	tag=dns json query answers | table query answers
};

tag=conns json SrcIP DstIP SrcIPBytes DstIPBytes | lookup -s -v -r @dns SrcIP answers query
| table SrcIP DstIP SrcIPBytes DstIPBytes
```

これにより、対応するDNSクエリを持たない接続だけのテーブルが得られます：

![](compound-ex2.png)

## コメント

クエリでは、クエリテキストの任意の場所に C スタイルのコメントを付けることができます。コメントは検索履歴に保存され、クエリのデバッグやインラインでのメモの追加に役立ちます。例えば、以下のようになります：

```
tag=foo json foo.bar /* 検索に影響を与えないＣ言語風のコメント */ baz | table
```
